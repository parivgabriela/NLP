{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook del Curso completo de NLP Parte 5\n",
        "\n",
        "Link al video de youtube:  \n",
        "https://youtu.be/9x1QtYNLJRY?si=Kfyw__RcNZFu941R&t=6000\n"
      ],
      "metadata": {
        "id": "s4aRrvBdkf_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Embeddings\n",
        "\n",
        "En el proceso de trabajar con textos en machine learning se debe transformar de texto a números, representados en vectores, que en el conjunto formarán matrices multidimensionales. Hasta ahora se abordó la vectorización de palabras, TF-IDF, pero nos queda por saber mucho más, en este nootebook algunas herramientas para reducir esa dimensionalidad y que sea eficiente con grandes volúmenes de texto.\n",
        "\n",
        "## ¿Qué es embedding?\n",
        "Embedding o incrustación es un medio para representar objetos como texto, imágenes y audio como puntos en un espacio vectorial continuo donde las ubicaciones de esos puntos en el espacio son semánticamente significativas para los algoritmos de machine learning (ML).\n",
        "\n",
        "A diferencia de otras técnicas de machine learning (ML), las incrustaciones se aprenden de datos que utilizan diversos algoritmos, como redes neuronales, en lugar de requerir explícitamente experiencia humana para definir. Permiten que el modelo aprenda patrones y relaciones complejas en los datos, que de otro modo serían imposibles de identificar para los humanos."
      ],
      "metadata": {
        "id": "ZKSI-nPvloi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Cómo funcionan las incrustaciones\n",
        "La mayoría de los algoritmos de aprendizaje automático solo pueden tomar datos numéricos de baja dimensión como entradas. Por lo tanto, es necesario convertir los datos en un formato numérico. Esto puede implicar cosas como crear una representación de \"bolsa de palabras\" para datos de texto, convertir imágenes en valores de píxeles o transformar datos gráficos en una matriz numérica.\n",
        "\n",
        "Los objetos que entran en un modelo incrustado se generan como incrustaciones, representados como vectores. Un vector es una matriz de números (p. ej. 1489, 22... 3, 777), donde cada número indica dónde se encuentra un objeto a lo largo de una dimensión especificada. El número de dimensiones puede llegar a mil o más dependiendo de la complejidad de los datos de entrada. Cuanto más cerca esté una incrustación de otras incrustaciones en este espacio n-dimensional, más similares serán. La similitud de distribución se determina por la longitud de los puntos vectoriales de un objeto al otro (medido según distancia euclidiana, coseno u otro).\n",
        "\n",
        "## Implementación:\n",
        "\n",
        "Se implementan típicamente como una capa de búsqueda (lookup table) dentro de una red neuronal. Esta capa es una matriz donde cada fila corresponde al vector (embedding) de un token o ID único.\n",
        "\n",
        "Los valores de estos vectores se aprenden a través del proceso de entrenamiento de la red neuronal, ajustándose para minimizar una función de pérdida específica de la tarea.\n",
        "\n",
        "## Ventajas:\n",
        "\n",
        "Mejor generalización al transferir el conocimiento de palabras o conceptos similares.\n",
        "\n",
        "Mayor eficiencia en el entrenamiento.\n",
        "\n"
      ],
      "metadata": {
        "id": "JTm8yhKsqIhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec es un modelo predictivo desarrollado por Google que genera Word Embeddings (vectores de palabras) a partir de grandes corpus de texto. Se basa en la hipótesis distribucional: las palabras que aparecen en contextos similares tienen significados similares.\n",
        "\n",
        "### ¿Cómo funciona?\n",
        "\n",
        "Entrena una red neuronal simple de dos capas para realizar una tarea de predicción de contexto. La \"magia\" son los pesos de la capa oculta, que se convierten en los vectores de palabras.\n",
        "\n",
        "### Tiene dos arquitecturas principales:\n",
        "\n",
        "Skip-Gram: Predice las palabras de contexto a partir de una palabra de entrada. (Mejor para corpus pequeños y palabras poco frecuentes).\n",
        "\n",
        "CBOW (Continuous Bag of Words): Predice la palabra objetivo a partir de las palabras de contexto que la rodean. (Más rápido de entrenar).\n",
        "\n",
        "Propiedades clave:\n",
        "\n",
        "Los vectores resultantes tienen propiedades algebraicas interesantes, permitiendo realizar analogías como:\n",
        "- vector(’Rey’)−vector(’Hombre’)+vector(’Mujer’)≈vector(’Reina’).\n"
      ],
      "metadata": {
        "id": "MgmwX7rHKc-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe (Global Vectors for Word Representation)\n",
        "\n",
        "GloVe es otro algoritmo para obtener Word Embeddings, desarrollado por la Universidad de Stanford. A diferencia de Word2Vec, que es un modelo puramente predictivo local, GloVe es un modelo basado en la factorización de matrices que combina información local y global.\n",
        "\n",
        "### ¿Cómo funciona?\n",
        "\n",
        "Se basa en el análisis de la Matriz de Co-ocurrencia de Palabras (X): Una matriz donde la entrada Xij​ cuenta cuántas veces la palabra j aparece en el contexto de la palabra i.\n",
        "\n",
        "El modelo GloVe entrena los vectores para que satisfagan una relación: el producto punto de dos vectores de palabras (wiT​wj​) debe ser igual al logaritmo de su frecuencia de co-ocurrencia (log(Xij​)).\n",
        "\n",
        "Minimiza una función de pérdida que depende de las frecuencias de co-ocurrencia, aprovechando así la información estadística global del corpus.\n",
        "\n",
        "### Ventajas:\n",
        "\n",
        "Combina las ventajas de los métodos basados en la predicción (como Word2Vec) y los métodos basados en la frecuencia/conteo (como la LSA, Latent Semantic Analysis), a menudo produciendo vectores con mejor rendimiento en tareas de analogía y similitud.\n",
        "\n",
        "### Ejemplo en Python (Carga de Vectores Pre-entrenados)\n",
        "\n",
        "GloVe se utiliza comúnmente cargando modelos pre-entrenados debido a los grandes recursos necesarios para entrenarlos desde cero. La librería Gensim se puede usar para cargarlos.\n"
      ],
      "metadata": {
        "id": "vqkK-uhRjPQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings Contextuales\n",
        "\n",
        "A pesar de su gran avance, modelos como Word2Vec y GloVe tienen una limitación fundamental: generan embeddings estáticos (o context-independent).\n",
        "\n",
        "### La Limitación de los Embeddings Estáticos\n",
        "En los modelos estáticos, a cada palabra se le asigna un único vector para todo el vocabulario del corpus, sin importar la frase o el contexto en el que aparezca.\n",
        "\n",
        "\n",
        "Esta limitación se vuelve evidente en el manejo de la polisemia (palabras con múltiples significados) o la homografía (palabras que se escriben igual pero tienen diferente significado o función gramatical).\n",
        "\n",
        "Ejemplo: La palabra \"banco\" tendrá el mismo vector en las siguientes dos oraciones:\n",
        "\n",
        "\"El banco central subió las tasas de interés.\" (Institución financiera).\n",
        "\n",
        "\"Vimos un banco de peces nadando juntos.\" (Grupo/cardumen).\n",
        "\n",
        "Dado que el vector es fijo, el modelo estático no puede distinguir el significado específico de la palabra en la frase.\n",
        "\n",
        "### La Evolución hacia los Embeddings Contextuales\n",
        "Los Embeddings Contextuales (o context-dependent) representan la siguiente generación en la representación del lenguaje. Estos modelos abordan la limitación estática generando un vector para cada palabra que es dinámico y se ajusta según el contexto de la oración completa en la que aparece.\n",
        "\n",
        "Esto permite que la palabra \"banco\" en la primera frase tenga un vector que la sitúe semánticamente cerca de \"dinero\" y \"tasas\", mientras que en la segunda frase tendrá un vector cercano a \"peces\" y \"nadando\"."
      ],
      "metadata": {
        "id": "rH0MLYnCjfaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelos Clave:\n",
        "El desarrollo de arquitecturas complejas, especialmente basadas en la arquitectura Transformer, ha permitido la creación de estos embeddings dinámicos:\n",
        "\n",
        "ELMo (Embeddings from Language Models): Fue uno de los primeros en usar modelos de lenguaje bidireccionales profundos para crear embeddings que variaban según el contexto de la oración.\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers): Un modelo altamente influyente que utiliza la arquitectura Transformer para pre-entrenar representaciones profundas y bidireccionales, generando embeddings que capturan el contexto de manera excepcional."
      ],
      "metadata": {
        "id": "vTMq5uhiqyHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demos\n"
      ],
      "metadata": {
        "id": "bZ6nW-W9i9O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTwq6I3_Nbs2",
        "outputId": "bc0b3890-f454-4a8a-ad18-d4b8fac319f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfiMzfA7iu_I",
        "outputId": "28ce3f01-8ad6-4272-a915-13ab2f4dbd81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Definir el Corpus de Texto\n",
        "2. Preprocesamiento (Tokenización). Dividimos el texto en oraciones y luego en palabras."
      ],
      "metadata": {
        "id": "wjn1ErarozBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvfMqaPPjkiP",
        "outputId": "85bfada3-feda-4725-9215-27fe70efb3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Primeras 2 oraciones tokenizadas ---\n",
            "[['word2vec', 'es', 'una', 'técnica', 'en', 'el', 'procesamiento', 'del', 'lenguaje', 'natural', '.'], ['aprende', 'representaciones', 'vectoriales', 'de', 'palabras', 'a', 'partir', 'de', 'grandes', 'corpus', '.']]\n",
            "----------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "corpus_raw = \"\"\"\n",
        "Word2Vec es una técnica en el procesamiento del lenguaje natural.\n",
        "Aprende representaciones vectoriales de palabras a partir de grandes corpus.\n",
        "Estas representaciones se llaman embeddings.\n",
        "El modelo Word2Vec es una red neuronal muy simple.\n",
        "Puede usarse para medir la similitud semántica entre palabras.\n",
        "\"\"\"\n",
        "# --- 2. Preprocesamiento (Tokenización) ---\n",
        "sentences_raw = sent_tokenize(corpus_raw.lower())\n",
        "\n",
        "# 2.2 Dividir cada oración en palabras (tokens)\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences_raw]\n",
        "\n",
        "print(\"--- Primeras 2 oraciones tokenizadas ---\")\n",
        "print(tokenized_sentences[:2])\n",
        "print(\"----------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Entrenamiento del Modelo Word2Vec\n",
        "    - El entrenamiento de esta \"red neuronal\" produce los embeddings.\n",
        "    - El resultado final son los pesos de la capa oculta."
      ],
      "metadata": {
        "id": "2RZDE5w3o-cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3\n",
        "\n",
        "# Parámetros clave:\n",
        "# - vector_size: La dimensión del vector de embedding (la capa oculta).\n",
        "# - window: El tamaño de la ventana de contexto (palabras alrededor de la palabra objetivo).\n",
        "# - min_count: Ignora todas las palabras con una frecuencia total menor a este valor.\n",
        "# - workers: Usa este número de hilos de trabajo para el entrenamiento.\n",
        "# - sg (Skip-gram): 1 para Skip-Gram, 0 para CBOW (default). Skip-Gram funciona mejor para corpus pequeños.\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_sentences,\n",
        "    vector_size=10,  # Dimensión de los embeddings (elegimos un número pequeño para el ejemplo)\n",
        "    window=2,        # Ventana de 2 palabras a cada lado\n",
        "    min_count=1,     # Incluir todas las palabras\n",
        "    sg=1             # Usar Skip-Gram\n",
        ")\n",
        "\n",
        "print(\"--- Modelo Word2Vec entrenado ---\")\n",
        "\n",
        "# --- 4. Obtener y Utilizar los Embeddings ---\n",
        "\n",
        "# a) Obtener el vector (embedding) de una palabra\n",
        "word_vector_lenguaje = model.wv['lenguaje']\n",
        "print(f\"\\nVector para 'lenguaje' (embedding):\\n{word_vector_lenguaje}\")\n",
        "print(f\"Dimensión del vector: {model.vector_size}\")\n",
        "\n",
        "\n",
        "# b) Encontrar palabras semánticamente similares\n",
        "# El modelo busca los vectores más cercanos en el espacio de embeddings.\n",
        "similar_words = model.wv.most_similar('lenguaje', topn=3)\n",
        "print(\"\\nPalabras más similares a 'lenguaje':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"- {word}: {similarity:.4f}\")\n",
        "\n",
        "\n",
        "# c) Analogías vectoriales (ejemplo famoso)\n",
        "# La operación se realiza sobre los vectores: Rey - Hombre + Mujer ≈ Reina\n",
        "# Aunque en nuestro corpus pequeño puede no funcionar bien:\n",
        "try:\n",
        "    analogy = model.wv.most_similar(positive=['procesamiento', 'vectorial'], negative=['natural'], topn=1)\n",
        "    print(f\"\\nAnalogía (procesamiento - natural + vectorial) ≈ {analogy[0][0]}\")\n",
        "except KeyError:\n",
        "    print(\"\\nNo se pueden calcular analogías por la limitación del vocabulario del ejemplo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lAC3-pUpB-Z",
        "outputId": "7dcdcebc-cbae-498f-9c87-4d298e43ce56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Modelo Word2Vec entrenado ---\n",
            "\n",
            "Vector para 'lenguaje' (embedding):\n",
            "[ 0.01671193 -0.02198509  0.09513601  0.09493855 -0.09774047  0.02505228\n",
            "  0.06156693  0.03872456  0.02022787  0.00430502]\n",
            "Dimensión del vector: 10\n",
            "\n",
            "Palabras más similares a 'lenguaje':\n",
            "- .: 0.7149\n",
            "- grandes: 0.6866\n",
            "- entre: 0.6470\n",
            "\n",
            "No se pueden calcular analogías por la limitación del vocabulario del ejemplo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Glove"
      ],
      "metadata": {
        "id": "Mq3phRE1N2BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Cargar un modelo GloVe pre-entrenado (por ejemplo, con 50 dimensiones en un corpus pequeño)\n",
        "# Nota: La primera vez, esto descargará el archivo.\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "    print(\"Modelo GloVe cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo (asegúrate de tener conexión a internet): {e}\")\n",
        "    # En un entorno de estudio, podrías mockear el objeto si fallara la descarga:\n",
        "    # glove_model = None\n",
        "\n",
        "if 'glove_model' in locals() and glove_model:\n",
        "    # 1. Obtener el vector para una palabra\n",
        "    palabra = 'computer'\n",
        "    vector_computadora = glove_model.get_vector(palabra)\n",
        "    print(f\"\\nVector para '{palabra}' (primeros 5 elementos):\")\n",
        "    print(vector_computadora[:5])\n",
        "\n",
        "    # 2. Encontrar palabras más similares\n",
        "    print(\"\\nPalabras más similares a 'technology':\")\n",
        "    similares = glove_model.most_similar('technology', topn=5)\n",
        "    for palabra, similitud in similares:\n",
        "        print(f\"- {palabra}: {similitud:.4f}\")\n",
        "\n",
        "    # 3. Realizar una analogía (King - Man + Woman = ?)\n",
        "    print(\"\\nAnalogía: 'Rey' - 'Hombre' + 'Mujer' = ?\")\n",
        "    resultado = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "    print(f\"Resultado: {resultado[0][0]}: {resultado[0][1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9lsZ7bENf_5",
        "outputId": "853722bc-7147-4046-b9ed-73c81be70b49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Modelo GloVe cargado exitosamente.\n",
            "\n",
            "Vector para 'computer' (primeros 5 elementos):\n",
            "[ 0.079084 -0.81504   1.7901    0.91653   0.10797 ]\n",
            "\n",
            "Palabras más similares a 'technology':\n",
            "- technologies: 0.8928\n",
            "- computer: 0.8526\n",
            "- systems: 0.8289\n",
            "- software: 0.8090\n",
            "- computing: 0.7991\n",
            "\n",
            "Analogía: 'Rey' - 'Hombre' + 'Mujer' = ?\n",
            "Resultado: queen: 0.8524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wJ_LXvlqgIhl"
      }
    }
  ]
}