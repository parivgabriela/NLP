{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook del Curso completo de NLP Parte 1\n",
        "\n",
        "Link al video de youtube:  \n",
        "https://www.youtube.com/watch?v=9x1QtYNLJRY&list=PL7HAy5R0ehQVdPVLV6pIJA9ZE2vVyLRxX&index=1\n"
      ],
      "metadata": {
        "id": "AgguKmN_gPTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bags of words\n",
        "\n",
        "Transforma la oración en un vector de las palabras, las transforma en números. Sirve para la detección de spam, análisis de sentimiento. No le da mucha importancia al orden de las palabras sino a si contiene ciertas palabras.\n",
        "\n",
        "## Método de conteo\n",
        "Este proceso permite determinar el tamaño del vocabulario. Es una de las formas más básicas y fundamentales de representar texto de manera numérica para que las computadoras puedan entenderlo.\n",
        "\n",
        "Ejemplo rápido\n",
        "\n",
        "Si tenemos dos frases:\n",
        "\n",
        "    \"El gato corre.\"\n",
        "    \"El perro corre.\"\n",
        "\n",
        "El vocabulario sería: [El, gato, perro, corre].\n",
        "\n",
        "    La frase 1 se representaría como: [1, 1, 0, 1]\n",
        "    La frase 2 se representaría como: [1, 0, 1, 1]"
      ],
      "metadata": {
        "id": "yNpMhZ0eh_dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización\n",
        "\n",
        "Divide el texto en tokens. Los tokens son unidades individuales de un texto.\n",
        "\n",
        "La tokenización es el primer paso crítico en cualquier flujo de trabajo de NLP. Básicamente, consiste en segmentar una cadena de texto en unidades más pequeñas llamadas tokens. Estos tokens pueden ser palabras, frases, subpalabras o incluso caracteres individuales.\n"
      ],
      "metadata": {
        "id": "D8gYaQHoLURb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LW3VEGixKjIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c72ffc5-ef95-479c-b128-bb0a30afac1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola!', 'Este', 'es', 'un', 'texto', 'en', 'español']\n"
          ]
        }
      ],
      "source": [
        "texto = 'Hola! Este es un texto en español'\n",
        "tokens = texto.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso la función separa por espacios, se puede especificar dentro de los parámetros, lo que al final produce es que junta los signos de puntuación, eso se puede analizar si se desea mantenerlo separado. En este caso juntos me indica que es una pregunta.\n",
        "\n",
        "Otra cosa que se puede hacer es cambiar las palabras a minúsculas para que luego se omitan las palabras iguales.\n",
        "\n",
        "Cuando se aplica la tokenización puede ser por palabra o por letra, cada letra/palabra tendra una representación numérica. La segunda puede ocupar mas espacio.\n",
        "\n",
        "### Observación\n",
        "\n",
        "Más adelante se trabajará con librerías que hacen el proceso de tokenización, este proceso tendra distinto comportamiento a la hora de procesar distintos lenguajes.\n",
        "\n",
        "**¿Qué cambia entre el Inglés y el Español?**\n",
        "\n",
        "Aunque ambos idiomas usan el alfabeto latino, existen diferencias lingüísticas que afectan cómo un algoritmo debe separar las palabras:\n",
        "\n",
        "Contracciones y Posesivos:\n",
        "\n",
        "- En inglés, es muy común encontrar formas como don't o it's. Un tokenizador debe decidir si separarlos como [\"do\", \"n't\"] o dejarlos juntos.\n",
        "\n",
        "- En español, las contracciones son mínimas (al, del), pero tenemos el reto de los pronombres enclíticos (verbos con pronombres pegados al final), como \"dámelo\". Un tokenizador avanzado debería ser capaz de entender que \"dámelo\" contiene tres unidades de significado: [\"da\", \"me\", \"lo\"].\n",
        "\n",
        "Morfología y Género:\n",
        "\n",
        "- El español es un idioma con mucha riqueza morfológica. Las palabras cambian de terminación según el género y número (gato, gata, gatos, gatas). En inglés, la mayoría de estas variaciones no existen (cat, cats). Esto hace que el vocabulario en español crezca más rápido si se tokeniza solo por palabras completas.\n",
        "\n",
        "Puntuación:\n",
        "\n",
        "- El español utiliza signos de apertura (¿, ¡), lo cual es una señal útil para el modelo que el inglés no posee. Un buen tokenizador debe tratar estos signos como tokens independientes para que el modelo aprenda la intención de la frase desde el inicio."
      ],
      "metadata": {
        "id": "_-AUedCpHi36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = texto.lower()\n",
        "tokens = texto.split()\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B1J0Ti5OKFH",
        "outputId": "b41818a4-1495-466b-d506-2bcffb176ba1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hola!', 'este', 'es', 'un', 'texto', 'en', 'español']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords\n",
        "\n",
        "Se identifican las palabras como advervios o artículos y luego se omiten.\n",
        "Esto permite reducir la dimensionalidad de los vectores."
      ],
      "metadata": {
        "id": "R-afgzZ-Krf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3D4mViWKtrU",
        "outputId": "06b85f48-9182-48c5-b7ce-f15b6fcd534c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = stopwords.words('spanish')\n",
        "print(f\"Long: {len(stop_words)}, stop_words: {sorted(stop_words)}\") # imprimo y lo muenstro ordenado alfabeticamente\n",
        "\n",
        "print(\"set long:\", len(set(stop_words)))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuoWf19SPhnj",
        "outputId": "e1276d62-3465-4683-cb35-6fbef7d9d4d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long: 313, stop_words: ['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis', 'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'sentid', 'sentida', 'sentidas', 'sentido', 'sentidos', 'seremos', 'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'siente', 'sin', 'sintiendo', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás', 'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos']\n",
            "set long: 313\n",
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"El gato es negro y el gato es blanco\"\n",
        "texto = texto.lower()\n",
        "\n",
        "tokens = word_tokenize(texto)\n",
        "texto_filtrado = [ word for word in tokens if word not in stop_words]\n",
        "print(\"Texto filtrado: \", texto_filtrado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa6HZbrIjO64",
        "outputId": "cf46aa01-cb3b-494f-c019-193df09ef638"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto filtrado:  ['gato', 'negro', 'gato', 'blanco']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming y Lematización\n",
        "\n",
        "Uno de los problemas de la tokenización de las palabras enteras es que entre palabras similares serán tomadas como entidades separadas que produce mayor dimensionalidad, tales como pájaro, pájaros y pájaritos, podrian ser asociados a la entidad de \"pájar\".\n",
        "\n",
        "El procceso de quitar el subfijo es el stemming, es un método más fácil\n",
        "\n",
        "La lematización es una técnica más sofisticada que utiliza las reglas gramaticales para obtener la base o raíz de la palabra.\n",
        "\n",
        "La lemmatización puede ser más efectiva que el stemmining pero es más costosa computacionalmente, también puede requerir un etiquetado previo.\n",
        "\n",
        "Aplicaciones del algoritmo\n",
        "\n",
        "- asistentes virtuales\n",
        "- analisis de sentimientos\n",
        "- motores de busqueda\n"
      ],
      "metadata": {
        "id": "p-x3qG1whqwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "r4nN2J1SLVOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989a2d5e-990b-43aa-c835-9a39d5ede485"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('spanish')\n",
        "print(stemmer.stem('caminando'))\n",
        "print(stemmer.stem('camino'))\n",
        "print(stemmer.stem('caminar'))\n"
      ],
      "metadata": {
        "id": "oSiXBui3LWFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e804a8-b1bd-4ac1-eefd-0c0ba8f08563"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "camin\n",
            "camin\n",
            "camin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -q\n"
      ],
      "metadata": {
        "id": "3WjF09n7LVsO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatización\n"
      ],
      "metadata": {
        "id": "GIjtOuZISyJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9fvBGXKsxQz",
        "outputId": "1f2a50be-6e61-4c1d-99e2-88e804a650fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "doc = nlp(\"bailaron bailando bailamos\")\n",
        "doc2 = nlp(\"pájaro pájaros pájarito pájarera\")\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text} -> {token.lemma_}\")\n",
        "print(\"-\"*18)\n",
        "for token in doc2:\n",
        "  print(f\"{token.text} -> {token.lemma_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFLQu9Fns5_5",
        "outputId": "a60d747b-00ec-4bab-834e-b3ae6a66595e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bailaron -> bailar\n",
            "bailando -> bailar\n",
            "bailamos -> bailamo\n",
            "------------------\n",
            "pájaro -> pájaro\n",
            "pájaros -> pájaro\n",
            "pájarito -> pájarito\n",
            "pájarera -> pájarero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumen\n",
        "\n",
        "En este laboratorio se abordo la primer parte del tutorial, como algunos términos del NLP, que se utilizan en distintas áreas como los chatbots, modelos de llm, generador de texto, clasificadores de texto, etc.\n",
        "\n",
        "El español posee un tipo de manipulación distinta, las acentuaciones, los artículos y el género de los sustantivos y adjetivos agregan dimensión al proceso de NLP, en algunos modelos de LLM los tokens en español se toma de a grupo de caracteres.\n",
        "\n",
        "Los algoritmos de Lemmatization y stemming permiten reducir la dimensión, esto a su vez reduce el error, según el criterio que se desea mejorar se puede optar por uno.\n",
        "\n",
        "La siguiente parte abordará un ejemplo práctico en un dataset."
      ],
      "metadata": {
        "id": "gOKWgeCGhKZ7"
      }
    }
  ]
}