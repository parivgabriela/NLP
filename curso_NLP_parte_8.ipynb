{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook del Curso completo de NLP Parte 8\n",
        "Link al video de youtube:\n",
        "https://youtu.be/9x1QtYNLJRY?si=Av7bvd07UEaMOU5s&t=9407"
      ],
      "metadata": {
        "id": "ixyZmQXG4NYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos probabilisticos - Procesos de Markov\n",
        "\n",
        "## Introducci√≥n al modelo\n",
        "\n",
        "Un Modelo de Markov es un proceso estoc√°stico (proceso aleatorio) que se define por la Propiedad de Markov. Esta propiedad establece que la probabilidad de transici√≥n a cualquier estado futuro depende solo del estado actual y no de la secuencia de eventos que lo precedieron. Dicho de forma m√°s formal, si Xn‚Äã representa el estado del sistema en el tiempo n, la propiedad de Markov se expresa como:\n",
        "\n",
        "$$P(Xn+1‚Äã=j‚à£X0‚Äã=i0‚Äã,X1‚Äã=i1‚Äã,‚Ä¶,Xn‚Äã=in‚Äã)=P(Xn+1‚Äã=j‚à£Xn‚Äã=in‚Äã)$$\n",
        "\n",
        "La forma m√°s b√°sica es la Cadena de Markov a Tiempo Discreto (CMTD), que consta de:\n",
        "\n",
        "- Un conjunto finito de estados (S): Los diferentes estados en los que puede estar el sistema.\n",
        "\n",
        "- Probabilidades de transici√≥n: La probabilidad de pasar del estado i al estado j en el siguiente paso, a menudo representadas en una Matriz de Transici√≥n P.\n",
        "\n",
        "Los modelos de Markov son ideales para modelar secuencias donde se asume una dependencia de corto alcance.\n",
        "\n",
        "\n",
        "## Propiedades Fundamentales\n",
        "\n",
        "Las cadenas de Markov tienen propiedades esenciales que definen su comportamiento:\n",
        "\n",
        "- Propiedad de Markov (Carencia de Memoria): Como se mencion√≥, el futuro solo depende del presente. Este es el rasgo definitorio del modelo.\n",
        "\n",
        "- No Negatividad: Todas las probabilidades de transici√≥n pij‚Äã deben ser mayores o iguales a cero (pij‚Äã‚â•0).\n",
        "\n",
        "- Suma de Probabilidades de Fila (Matriz Estoc√°stica): La suma de las probabilidades de transici√≥n salientes de cualquier estado debe ser igual a uno. Si P es la matriz de transici√≥n:\n",
        "$$j‚ààS‚àë‚ÄãP(X_n+1‚Äã=j‚à£X_n‚Äã=i)=1$$\n",
        "\n",
        "- Distribuci√≥n Estacionaria (o de Estado Estable): Para ciertas cadenas (irreducibles y aperi√≥dicas), existe una distribuci√≥n de probabilidad œÄ de los estados a largo plazo que no cambia con el tiempo. El sistema converge a este comportamiento a largo plazo, independientemente del estado inicial.\n",
        "\n",
        "- Irreducibilidad: Una cadena es irreducible si es posible pasar de cualquier estado a cualquier otro estado (no necesariamente en un solo paso).\n",
        "\n",
        "- Aperiodicidad: Una cadena es aperi√≥dica si no hay un patr√≥n c√≠clico fijo para regresar a un estado."
      ],
      "metadata": {
        "id": "XM7ybLd1bxH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicaci√≥n en el Procesamiento del Lenguaje Natural (NLP)\n",
        "\n",
        "Los modelos de Markov, y en particular los Modelos Ocultos de Markov (HMM), han sido hist√≥ricamente cruciales en el campo del NLP.\n",
        "\n",
        "Las cadenas de Markov b√°sicas se emplean en tareas como:\n",
        "\n",
        "- Generaci√≥n Autom√°tica de Texto: Se utiliza la probabilidad de que una palabra le siga a otra (modelos de n-gramas) para generar secuencias de texto que imitan el estilo de un corpus de entrenamiento.\n",
        "\n",
        "- Modelado del Lenguaje: Estimar la probabilidad de una secuencia de palabras.\n",
        "- Clasificaci√≥n de texto: puede ser utilizado para detectar spam.\n",
        "- Diferencia entre aprendizaje supervisado y No supervisado: Cuando se utilizo el modelo con los textos de noticias se entreno con datos etiquetados, pero en el no supervisado\n",
        "\n",
        "### Uso de Modelos Ocultos de Markov (HMM)\n",
        "\n",
        "El HMM es una extensi√≥n m√°s compleja que permite modelar procesos donde los estados no son directamente observables (son \"ocultos\"). Un HMM est√° definido por:\n",
        "\n",
        "- Estados Ocultos (S): El proceso subyacente que no se puede observar directamente (ej. las etiquetas gramaticales -sustantivo, verbo- en una oraci√≥n).\n",
        "\n",
        "- S√≠mbolos Observables (V): La secuencia de eventos que podemos ver (ej. las palabras de la oraci√≥n).\n",
        "\n",
        "- Probabilidades de Transici√≥n (A): La probabilidad de cambiar de un estado oculto a otro (ej. P(Verbo‚à£Sustantivo)).\n",
        "\n",
        "- Probabilidades de Emisi√≥n (B): La probabilidad de observar un s√≠mbolo dado un estado oculto (ej. P(\"correr\"‚à£Verbo)).\n",
        "\n",
        "- Probabilidades Iniciales (œÄ): La probabilidad de iniciar en un estado oculto.\n",
        "\n",
        "### Las principales aplicaciones de los HMM en NLP incluyen:\n",
        "\n",
        "- Etiquetado de Partes del Discurso (POS Tagging): Determinar la categor√≠a gramatical de cada palabra en una oraci√≥n. Los estados ocultos son las etiquetas POS y las observaciones son las palabras.\n",
        "\n",
        "- Reconocimiento del Habla: Los HMM fueron una tecnolog√≠a dominante. Modelan la secuencia de fonemas o estados ac√∫sticos subyacentes (ocultos) que generan la se√±al de voz (observable).\n",
        "\n",
        "- Traducci√≥n Autom√°tica: Se usaron en los primeros sistemas de traducci√≥n estad√≠stica para modelar la alineaci√≥n entre palabras en diferentes idiomas.\n",
        "\n",
        "- Reconocimiento de Entidades Nombradas (NER): Identificar nombres de personas, lugares u organizaciones en el texto."
      ],
      "metadata": {
        "id": "WKHE-qvLeScg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librer√≠as Esenciales para Modelos de Markov en PLN\n",
        "\n",
        "### hmmlearn\n",
        "*hmmlearn* es la librer√≠a de facto para trabajar con Modelos Ocultos de Markov en Python, dise√±ada para integrarse bien con el ecosistema de scikit-learn. Es robusta y permite implementar HMMs Gaussianos, Multinomiales, y Categ√≥ricos."
      ],
      "metadata": {
        "id": "DuzEaM2z-lXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# 1. Definici√≥n del Modelo\n",
        "# N_components es el n√∫mero de estados (e.g., Noun, Verb, Adj, etc.)\n",
        "n_states = 3\n",
        "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)"
      ],
      "metadata": {
        "id": "j6sD9aWPbkuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nltk - Natural Language Toolkit\n",
        "\n",
        "Aunque NLTK no tiene una implementaci√≥n dedicada y altamente optimizada como hmmlearn, incluye un m√≥dulo fundamental que ya contiene un HMM entrenado para Etiquetado POS y es excelente para la documentaci√≥n did√°ctica y la experimentaci√≥n."
      ],
      "metadata": {
        "id": "U7_VzsRT-kx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# nltk.download('averaged_perceptron_tagger') # Necesario si no est√° descargado\n",
        "# nltk.download('punkt') # Necesario si no est√° descargado\n",
        "\n",
        "# 1. Oraci√≥n de ejemplo\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# 2. Tokenizar\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 3. Etiquetado (POS Tagging)\n",
        "# NLTK utiliza un modelo estad√≠stico (a menudo un HMM o Perceptron)\n",
        "# para asignar la etiqueta POS m√°s probable a cada palabra.\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "print(tagged_tokens)\n",
        "# Salida esperada (ejemplo): [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ...]\n",
        "# 'DT': Determinante, 'JJ': Adjetivo, 'NN': Sustantivo"
      ],
      "metadata": {
        "id": "aylRwVY_AWAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pomegranate\n",
        "\n",
        "pomegranate es una librer√≠a que implementa modelos probabil√≠sticos y de secuencias, incluyendo HMMs, de una manera muy flexible y a menudo con mejor rendimiento que hmmlearn en ciertas tareas, ya que est√° optimizada para el c√°lculo de probabilidades.\n",
        "üõ†Ô∏è Demostraci√≥n: Creaci√≥n y Visualizaci√≥n de un HMM (Conceptual)\n",
        "\n",
        "Es particularmente √∫til para documentar c√≥mo se construyen los estados y las transiciones de un HMM desde cero.\n",
        "\n",
        "Bloque de C√≥digo Conceptual:"
      ],
      "metadata": {
        "id": "Pkx0i2vmBrLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pomegranate import HiddenMarkovModel, State, DiscreteDistribution\n",
        "\n",
        "# 1. Definir el conjunto de Observaciones (Palabras)\n",
        "# En un HMM Multinomial, las observaciones son discretas.\n",
        "palabras = [\"camin√≥\", \"perro\"]\n",
        "\n",
        "# 2. Definir las Distribuciones de Emisi√≥n (B) para cada Estado (Etiqueta POS)\n",
        "\n",
        "# Estado: SUSTANTIVO (Noun)\n",
        "# Probabilidades de que un Noun \"emita\" las palabras observadas.\n",
        "# P(\"perro\" | Noun) es alta; P(\"camin√≥\" | Noun) es baja.\n",
        "dist_noun = DiscreteDistribution({\n",
        "    \"perro\": 0.9,\n",
        "    \"camin√≥\": 0.1\n",
        "})\n",
        "state_noun = State(dist_noun, name=\"NOUN\")\n",
        "\n",
        "# Estado: VERBO (Verb)\n",
        "# Probabilidades de que un Verb \"emita\" las palabras observadas.\n",
        "# P(\"camin√≥\" | Verb) es alta; P(\"perro\" | Verb) es baja.\n",
        "dist_verb = DiscreteDistribution({\n",
        "    \"perro\": 0.2,\n",
        "    \"camin√≥\": 0.8\n",
        "})\n",
        "state_verb = State(dist_verb, name=\"VERB\")\n",
        "\n",
        "# 3. Crear el Modelo HMM\n",
        "model = HiddenMarkovModel(name=\"Simple POS Tagger\")\n",
        "\n",
        "# 4. Agregar los Estados\n",
        "model.add_states(state_noun, state_verb)\n",
        "\n",
        "# 5. Definir las Probabilidades de Transici√≥n (A) y de Inicio (Pi)\n",
        "\n",
        "# --- Probabilidades de Inicio (Pi) ---\n",
        "# Empezar con un Noun es m√°s probable.\n",
        "model.add_transition(model.start, state_noun, 0.8)\n",
        "model.add_transition(model.start, state_verb, 0.2)\n",
        "\n",
        "# --- Probabilidades de Transici√≥n (A) ---\n",
        "# Noun -> Verb (0.6), Noun -> Noun (0.4)\n",
        "model.add_transition(state_noun, state_verb, 0.6)\n",
        "model.add_transition(state_noun, state_noun, 0.4)\n",
        "\n",
        "# Verb -> Noun (0.7), Verb -> Verb (0.3)\n",
        "model.add_transition(state_verb, state_noun, 0.7)\n",
        "model.add_transition(state_verb, state_verb, 0.3)\n",
        "\n",
        "# 6. Finalizar el Modelo\n",
        "model.bake()\n",
        "\n",
        "# 7. Decodificaci√≥n (Etiquetado) usando el algoritmo de Viterbi\n",
        "# Oraci√≥n: \"perro camin√≥ perro\"\n",
        "sequence = [\"perro\", \"camin√≥\", \"perro\"]\n",
        "\n",
        "# El m√©todo `viterbi` devuelve el logaritmo de la probabilidad\n",
        "# de la secuencia de estados m√°s probable y la secuencia de estados (tags).\n",
        "log_prob, path = model.viterbi(sequence)\n",
        "\n",
        "# Extraer solo la secuencia de nombres de estado (tags)\n",
        "predicted_tags = [state.name for index, state in path[1:-1]]\n",
        "\n",
        "print(f\"Secuencia de Observaci√≥n: {sequence}\")\n",
        "print(f\"Secuencia de Etiquetas (Viterbi): {predicted_tags}\")\n",
        "# Salida esperada (con estos par√°metros): ['NOUN', 'VERB', 'NOUN']"
      ],
      "metadata": {
        "id": "2B84QmpiBr2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}